### Node爬虫流程
爬虫的概念就是一种按照一定的规则，自动的抓取网站信息的程序或者脚本

#### 1. 抓取
最重要的步骤，需要把所需要的页面抓取出来，同时需要兼顾时间效率，能够并发的同时爬取多个页面。
需要分析页面结构，按需求爬取所需要的数据。
#### 2. 分析
对网页进行文本分析。通常分析和存储会交替进行。
#### 3. 存储
将页面内容会先使用一定的策略进行存储。这次的Demo使用的是mongod。
#### 4. 展示
根据需求将爬取的结果数据进行展示。

##### 结合Demo的流程
###### step 1 获取博文文章入口URL
目标是爬取博客园博客里面博主信息。首先需要爬取每一页的博文列表，然后再依次请求博文列表数据，获取博主的用户ID之后再请求数据获取详情。

> eventproxy
eventproxy(https://github.com/JacksonTian/eventproxy ) 非常轻量的工具，但是能够带来一种事件式编程的思维变化。

用 js 写过异步的同学应该都知道，如果你要并发异步获取两三个地址的数据，并且要在获取到数据之后，对这些数据一起进行利用的话，常规的写法是自己维护一个计数器。  

先定义一个 var count =0，然后每次抓取成功以后，就 count++。如果你是要抓取三个源的数据，由于你根本不知道这些异步操作到底谁先完成，那么每次当抓取成功的时候，就判断一下count ===3。当值为真时，使用另一个函数继续完成操作。

而 eventproxy就起到了这个计数器的作用，它来帮你管理到底这些异步操作是否完成，完成之后，它会自动调用你提供的处理函数，并将抓取到的数据当参数传过来。

###### step 2 爬取具体的页面，使用async控制异步并发数量。

> async
async(https://github.com/caolan/async#queueworker-concurrency)，async是一个流程控制工具包，提供了直接而强大的异步功能mapLimit(arr, limit, iterator, callback)。

这次我们要介绍的是 async 的 mapLimit(arr, limit, iterator, callback) 接口。
这个方法也是就说，传入请求接口的集合参数，以及并发数量，比如一次请求5个接口，只有在这5个接口全部异步请求完成之后，才可以继续进行接下来的请求。
